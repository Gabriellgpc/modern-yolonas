{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Post-Training Quantization (PTQ)\n",
        "\n",
        "This tutorial demonstrates how to quantize a trained YOLO-NAS model to INT8 using PTQ with calibration data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "You need a trained checkpoint and calibration data (typically the validation set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "from modern_yolonas import yolo_nas_s\n",
        "from modern_yolonas.data import YOLODetectionDataset, load_dataset_config\n",
        "from modern_yolonas.data.transforms import Compose, LetterboxResize, Normalize\n",
        "from modern_yolonas.training import extract_model_state_dict, DetectionDataModule\n",
        "from modern_yolonas.quantization import (\n",
        "    prepare_model_ptq, run_calibration, convert_quantized, export_quantized_onnx,\n",
        ")\n",
        "\n",
        "# Load trained model\n",
        "model = yolo_nas_s(pretrained=True)  # or load from checkpoint\n",
        "# sd = extract_model_state_dict(\"runs/hardhat/last.ckpt\")\n",
        "# model.load_state_dict(sd)\n",
        "model.eval()\n",
        "\n",
        "# Setup calibration data\n",
        "# cfg = load_dataset_config(\"path/to/data.yaml\")\n",
        "# val_transforms = Compose([LetterboxResize(target_size=640), Normalize()])\n",
        "# val_ds = YOLODetectionDataset(root=cfg.root, split=cfg.val_split, transforms=val_transforms)\n",
        "# data_module = DetectionDataModule(train_dataset=val_ds, val_dataset=val_ds, batch_size=8, num_workers=2)\n",
        "# val_loader = data_module.val_dataloader()\n",
        "\n",
        "# Prepare model for PTQ (inserts observers)\n",
        "ptq_model = prepare_model_ptq(model)\n",
        "print(\"PTQ model prepared with observers\")\n",
        "\n",
        "# Calibrate on validation set\n",
        "# run_calibration(ptq_model, val_loader, num_batches=20, device=\"cpu\")\n",
        "# print(\"Calibration complete\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to quantized model\n",
        "quantized_model = convert_quantized(ptq_model)\n",
        "print(\"Model converted to INT8\")\n",
        "\n",
        "# Export quantized ONNX\n",
        "ptq_onnx_path = \"model_ptq_int8.onnx\"\n",
        "export_quantized_onnx(quantized_model, ptq_onnx_path, input_size=640)\n",
        "\n",
        "ptq_size = Path(ptq_onnx_path).stat().st_size / (1024 * 1024)\n",
        "print(f\"\\nPTQ ONNX exported to: {ptq_onnx_path}\")\n",
        "print(f\"PTQ model size: {ptq_size:.1f} MB\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
