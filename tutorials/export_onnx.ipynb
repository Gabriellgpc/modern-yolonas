{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Export YOLO-NAS to ONNX\n",
        "\n",
        "This tutorial demonstrates how to export a trained YOLO-NAS checkpoint to ONNX format for deployment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from pathlib import Path\n",
        "from modern_yolonas import yolo_nas_s\n",
        "from modern_yolonas.training import extract_model_state_dict\n",
        "\n",
        "# Load model from checkpoint\n",
        "# Replace with your checkpoint path, or use pretrained=True for COCO weights\n",
        "model = yolo_nas_s(pretrained=True)\n",
        "# sd = extract_model_state_dict(\"runs/hardhat/last.ckpt\")\n",
        "# model.load_state_dict(sd)\n",
        "model.eval()\n",
        "\n",
        "# Fuse RepVGG blocks for deployment (merges BN into conv)\n",
        "for module in model.modules():\n",
        "    if hasattr(module, \"fuse_block_residual_branches\"):\n",
        "        module.fuse_block_residual_branches()\n",
        "\n",
        "dummy = torch.randn(1, 3, 640, 640)\n",
        "onnx_path = \"model_float32.onnx\"\n",
        "\n",
        "torch.onnx.export(\n",
        "    model, dummy, onnx_path,\n",
        "    input_names=[\"images\"],\n",
        "    output_names=[\"pred_bboxes\", \"pred_scores\"],\n",
        "    dynamic_axes={\n",
        "        \"images\": {0: \"batch\"},\n",
        "        \"pred_bboxes\": {0: \"batch\"},\n",
        "        \"pred_scores\": {0: \"batch\"},\n",
        "    },\n",
        "    opset_version=17,\n",
        ")\n",
        "\n",
        "onnx_size = Path(onnx_path).stat().st_size / (1024 * 1024)\n",
        "print(f\"ONNX exported to: {onnx_path}\")\n",
        "print(f\"Float32 model size: {onnx_size:.1f} MB\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "session = ort.InferenceSession(onnx_path)\n",
        "outputs = session.run(None, {\"images\": dummy.numpy()})\n",
        "print(f\"Output shapes: bboxes={outputs[0].shape}, scores={outputs[1].shape}\")\n",
        "print(\"ONNX model verified successfully!\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
