{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning YOLO-NAS on a Roboflow Dataset\n",
    "\n",
    "This notebook demonstrates the full pipeline:\n",
    "\n",
    "1. Download a dataset from Roboflow\n",
    "2. Explore the data\n",
    "3. Fine-tune YOLO-NAS S with pretrained backbone\n",
    "4. Evaluate and visualize results\n",
    "5. Export to ONNX\n",
    "6. Post-Training Quantization (PTQ)\n",
    "7. Quantization-Aware Training (QAT)\n",
    "8. ONNX Runtime inference\n",
    "\n",
    "**Requirements:** GPU recommended. Install `roboflow` for dataset download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Download\n",
    "# Install roboflow if not already installed\n",
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Replace with your Roboflow API key\n",
    "# Get one free at https://roboflow.com\n",
    "rf = Roboflow(api_key=\"<YOUR_API_KEY>\")\n",
    "\n",
    "# Download Hard Hat Workers dataset (small, 3 classes)\n",
    "# You can replace this with any Roboflow dataset\n",
    "project = rf.workspace(\"lus-gabriel\").project(\"hard-hat-sample-5g5ip\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov5\")\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1b: Parse data.yaml\n",
    "from pathlib import Path\n",
    "from modern_yolonas.data import load_dataset_config, YOLODetectionDataset\n",
    "\n",
    "# Parse the Roboflow data.yaml\n",
    "cfg = load_dataset_config(\"./hardhat-dataset/data.yaml\")\n",
    "\n",
    "print(f\"Dataset root:  {cfg.root}\")\n",
    "print(f\"Num classes:   {cfg.num_classes}\")\n",
    "print(f\"Class names:   {cfg.class_names}\")\n",
    "print(f\"Train split:   {cfg.train_split}\")\n",
    "print(f\"Val split:     {cfg.val_split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Explore Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load raw dataset (no transforms) for exploration\n",
    "train_raw = YOLODetectionDataset(root=cfg.root, split=cfg.train_split)\n",
    "val_raw = YOLODetectionDataset(root=cfg.root, split=cfg.val_split)\n",
    "\n",
    "print(f\"Train images: {len(train_raw)}\")\n",
    "print(f\"Val images:   {len(val_raw)}\")\n",
    "\n",
    "# Count class distribution across training set\n",
    "class_counts = Counter()\n",
    "for i in range(len(train_raw)):\n",
    "    _, targets = train_raw.load_raw(i)\n",
    "    for t in targets:\n",
    "        class_counts[int(t[0])] += 1\n",
    "\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "for cls_id, count in sorted(class_counts.items()):\n",
    "    name = cfg.class_names[cls_id] if cls_id < len(cfg.class_names) else f\"class_{cls_id}\"\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for ax, idx in zip(axes, range(4)):\n",
    "    img, targets = train_raw.load_raw(idx)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for t in targets:\n",
    "        cls_id, cx, cy, bw, bh = t\n",
    "        x1 = int((cx - bw / 2) * w)\n",
    "        y1 = int((cy - bh / 2) * h)\n",
    "        x2 = int((cx + bw / 2) * w)\n",
    "        y2 = int((cy + bh / 2) * h)\n",
    "        color = [(0, 255, 0), (255, 0, 0), (0, 0, 255)][int(cls_id) % 3]\n",
    "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color, 2)\n",
    "        label = cfg.class_names[int(cls_id)] if int(cls_id) < len(cfg.class_names) else str(int(cls_id))\n",
    "        cv2.putText(img_rgb, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    ax.imshow(img_rgb)\n",
    "    ax.set_title(f\"Image {idx} ({len(targets)} objects)\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Fine-Tune YOLO-NAS S\nimport torch\nimport lightning as L\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\nfrom modern_yolonas import yolo_nas_s\nfrom modern_yolonas.data import YOLODetectionDataset\nfrom modern_yolonas.data.transforms import (\n    Compose,\n    HSVAugment,\n    HorizontalFlip,\n    LetterboxResize,\n    Normalize,\n)\nfrom modern_yolonas.training import (\n    YoloNASLightningModule,\n    EMACallback,\n    DetectionDataModule,\n)\n\n# Build model with pretrained COCO backbone, random head for our classes\nmodel = yolo_nas_s(pretrained=True, num_classes=cfg.num_classes)\nprint(f\"Model created: YOLO-NAS S with {cfg.num_classes} classes\")\nprint(\"Backbone+neck weights: loaded from COCO pretrained\")\nprint(\"Head cls_pred layers: randomly initialized\")\n\n# Transforms\ntrain_transforms = Compose([\n    HSVAugment(),\n    HorizontalFlip(p=0.5),\n    LetterboxResize(target_size=640),\n    Normalize(),\n])\nval_transforms = Compose([\n    LetterboxResize(target_size=640),\n    Normalize(),\n])\n\n# Datasets\ntrain_ds = YOLODetectionDataset(\n    root=cfg.root, split=cfg.train_split, transforms=train_transforms\n)\nval_ds = YOLODetectionDataset(\n    root=cfg.root, split=cfg.val_split, transforms=val_transforms\n)\n\n# Lightning training\nlit_model = YoloNASLightningModule(\n    model=model, num_classes=cfg.num_classes, lr=2e-4, warmup_steps=100,\n)\ndata_module = DetectionDataModule(\n    train_dataset=train_ds, val_dataset=val_ds, batch_size=8, num_workers=2,\n)\n\ntrainer = L.Trainer(\n    max_epochs=15,\n    accelerator=\"auto\",\n    precision=\"16-mixed\",\n    callbacks=[EMACallback(), ModelCheckpoint(dirpath=\"runs/hardhat\", save_last=True)],\n    default_root_dir=\"runs/hardhat\",\n)\ntrainer.fit(lit_model, datamodule=data_module)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Evaluate & Visualize\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom modern_yolonas import Detector\nfrom modern_yolonas.training import extract_model_state_dict\n\n# Load best checkpoint (handles both Lightning .ckpt and legacy .pt)\nbest_ckpt = \"runs/hardhat/last.ckpt\"\nsd = extract_model_state_dict(best_ckpt)\nmodel.load_state_dict(sd)\nmodel.eval()\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndetector = Detector(\n    model=model,\n    device=device,\n    conf_threshold=0.25,\n    iou_threshold=0.45,\n    retain_image=True,\n)\n\n# Run inference on validation images\nval_images = sorted((cfg.root / \"images\" / cfg.val_split).glob(\"*.*\"))[:8]\n\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\nclass_detection_counts = Counter()\n\nfor ax, img_path in zip(axes.flat, val_images):\n    detection = detector.detect_image(str(img_path))\n    vis = detection.visualize(class_names=cfg.class_names)\n    ax.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n    ax.set_title(f\"{len(detection.boxes)} detections\")\n    ax.axis(\"off\")\n\n    for cls_id in detection.class_ids:\n        name = cfg.class_names[int(cls_id)] if int(cls_id) < len(cfg.class_names) else f\"class_{int(cls_id)}\"\n        class_detection_counts[name] += 1\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nDetection counts per class:\")\nfor name, count in class_detection_counts.most_common():\n    print(f\"  {name}: {count}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Export to ONNX\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "export_model = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\n",
    "export_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "export_model.eval()\n",
    "\n",
    "# Fuse RepVGG blocks for deployment\n",
    "for module in export_model.modules():\n",
    "    if hasattr(module, \"fuse_block_residual_branches\"):\n",
    "        module.fuse_block_residual_branches()\n",
    "\n",
    "dummy = torch.randn(1, 3, 640, 640)\n",
    "onnx_path = \"runs/hardhat/model_float32.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    dummy,\n",
    "    onnx_path,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"pred_bboxes\", \"pred_scores\"],\n",
    "    dynamic_axes={\n",
    "        \"images\": {0: \"batch\"},\n",
    "        \"pred_bboxes\": {0: \"batch\"},\n",
    "        \"pred_scores\": {0: \"batch\"},\n",
    "    },\n",
    "    opset_version=17,\n",
    ")\n",
    "\n",
    "onnx_size = Path(onnx_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"ONNX exported to: {onnx_path}\")\n",
    "print(f\"Float32 model size: {onnx_size:.1f} MB\")\n",
    "\n",
    "# Verify output shapes\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "outputs = session.run(None, {\"images\": dummy.numpy()})\n",
    "print(f\"Output shapes: bboxes={outputs[0].shape}, scores={outputs[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Post-Training Quantization (PTQ)\n",
    "from modern_yolonas.quantization import (\n",
    "    prepare_model_ptq,\n",
    "    run_calibration,\n",
    "    convert_quantized,\n",
    "    export_quantized_onnx,\n",
    ")\n",
    "\n",
    "# Prepare model for PTQ\n",
    "ptq_model_fresh = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\n",
    "ptq_model_fresh.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "ptq_model_fresh.eval()\n",
    "\n",
    "ptq_model = prepare_model_ptq(ptq_model_fresh)\n",
    "print(\"PTQ model prepared with observers\")\n",
    "\n",
    "# Calibrate on validation set (small set is fine for PTQ)\n",
    "run_calibration(ptq_model, val_loader, num_batches=20, device=\"cpu\")\n",
    "print(\"Calibration complete\")\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = convert_quantized(ptq_model)\n",
    "print(\"Model converted to int8\")\n",
    "\n",
    "# Export quantized ONNX\n",
    "ptq_onnx_path = \"runs/hardhat/model_ptq_int8.onnx\"\n",
    "export_quantized_onnx(quantized_model, ptq_onnx_path, input_size=640)\n",
    "\n",
    "ptq_size = Path(ptq_onnx_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nPTQ ONNX exported to: {ptq_onnx_path}\")\n",
    "print(f\"PTQ model size: {ptq_size:.1f} MB\")\n",
    "print(f\"Size reduction: {(1 - ptq_size / onnx_size) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Quantization-Aware Training (QAT)\nimport lightning as L\nfrom modern_yolonas.quantization import prepare_model_qat\nfrom modern_yolonas.training import YoloNASLightningModule, QATCallback, DetectionDataModule\n\n# Prepare model for QAT from the fine-tuned checkpoint\nqat_model_fresh = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\nsd = extract_model_state_dict(best_ckpt)\nqat_model_fresh.load_state_dict(sd)\n\nqat_model = prepare_model_qat(qat_model_fresh)\nprint(\"QAT model prepared with fake-quantization nodes\")\n\n# QAT fine-tuning (short, low LR, no AMP)\nqat_lit_model = YoloNASLightningModule(\n    model=qat_model, num_classes=cfg.num_classes, lr=2e-5, warmup_steps=50,\n)\nqat_data_module = DetectionDataModule(\n    train_dataset=train_ds, val_dataset=val_ds, batch_size=8, num_workers=2,\n)\n\nqat_trainer = L.Trainer(\n    max_epochs=5,\n    accelerator=\"auto\",\n    precision=\"32-true\",  # No AMP â€” fake-quant incompatible with autocast\n    callbacks=[QATCallback(freeze_bn_after_epoch=3, freeze_observer_after_epoch=5)],\n    default_root_dir=\"runs/hardhat/qat\",\n)\nqat_trainer.fit(qat_lit_model, datamodule=qat_data_module)\nprint(\"QAT training complete!\")\n\n# Convert QAT model and export\nqat_quantized = convert_quantized(qat_model)\nqat_onnx_path = \"runs/hardhat/model_qat_int8.onnx\"\nexport_quantized_onnx(qat_quantized, qat_onnx_path, input_size=640)\n\nqat_size = Path(qat_onnx_path).stat().st_size / (1024 * 1024)\nprint(f\"\\nQAT ONNX exported to: {qat_onnx_path}\")\nprint(f\"\\nModel size comparison:\")\nprint(f\"  Float32: {onnx_size:.1f} MB\")\nprint(f\"  PTQ:     {ptq_size:.1f} MB ({(1 - ptq_size / onnx_size) * 100:.0f}% smaller)\")\nprint(f\"  QAT:     {qat_size:.1f} MB ({(1 - qat_size / onnx_size) * 100:.0f}% smaller)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: ONNX Runtime Inference\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modern_yolonas.inference.preprocess import preprocess\n",
    "from modern_yolonas.inference.postprocess import postprocess, rescale_boxes\n",
    "from modern_yolonas.inference.visualize import draw_detections\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Load a test image\n",
    "test_image_path = str(val_images[0])\n",
    "original = cv2.imread(test_image_path)\n",
    "orig_h, orig_w = original.shape[:2]\n",
    "\n",
    "# Preprocess\n",
    "input_tensor, pad_info = preprocess(original, input_size=640)\n",
    "\n",
    "# Run inference\n",
    "pred_bboxes, pred_scores = session.run(\n",
    "    None, {\"images\": input_tensor.numpy()}\n",
    ")\n",
    "\n",
    "# Postprocess\n",
    "import torch\n",
    "boxes, scores, class_ids = postprocess(\n",
    "    torch.from_numpy(pred_bboxes),\n",
    "    torch.from_numpy(pred_scores),\n",
    "    conf_threshold=0.25,\n",
    "    iou_threshold=0.45,\n",
    ")\n",
    "\n",
    "# Rescale boxes to original image size\n",
    "if len(boxes) > 0:\n",
    "    boxes = rescale_boxes(boxes, pad_info, (orig_h, orig_w))\n",
    "\n",
    "# Visualize\n",
    "annotated = draw_detections(\n",
    "    original, boxes.numpy(), scores.numpy(), class_ids.numpy(),\n",
    "    class_names=cfg.class_names,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"ONNX Runtime Inference ({len(boxes)} detections)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDetections:\")\n",
    "for i in range(len(boxes)):\n",
    "    cls_name = cfg.class_names[int(class_ids[i])] if int(class_ids[i]) < len(cfg.class_names) else f\"class_{int(class_ids[i])}\"\n",
    "    print(f\"  {cls_name}: {scores[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}