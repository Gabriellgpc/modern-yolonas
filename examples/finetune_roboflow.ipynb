{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning YOLO-NAS on a Roboflow Dataset\n",
    "\n",
    "This notebook demonstrates the full pipeline:\n",
    "\n",
    "1. Download a dataset from Roboflow\n",
    "2. Explore the data\n",
    "3. Fine-tune YOLO-NAS S with pretrained backbone\n",
    "4. Evaluate and visualize results\n",
    "5. Export to ONNX\n",
    "6. Post-Training Quantization (PTQ)\n",
    "7. Quantization-Aware Training (QAT)\n",
    "8. ONNX Runtime inference\n",
    "\n",
    "**Requirements:** GPU recommended. Install `roboflow` for dataset download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Download\n",
    "# Install roboflow if not already installed\n",
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "\n",
    "# Replace with your Roboflow API key\n",
    "# Get one free at https://roboflow.com\n",
    "rf = Roboflow(api_key=\"<YOUR_API_KEY>\")\n",
    "\n",
    "# Download Hard Hat Workers dataset (small, 3 classes)\n",
    "# You can replace this with any Roboflow dataset\n",
    "project = rf.workspace(\"lus-gabriel\").project(\"hard-hat-sample-5g5ip\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov5\")\n",
    "\n",
    "print(f\"Dataset downloaded to: {dataset.location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1b: Parse data.yaml\n",
    "from pathlib import Path\n",
    "from modern_yolonas.data import load_dataset_config, YOLODetectionDataset\n",
    "\n",
    "# Parse the Roboflow data.yaml\n",
    "cfg = load_dataset_config(\"./hardhat-dataset/data.yaml\")\n",
    "\n",
    "print(f\"Dataset root:  {cfg.root}\")\n",
    "print(f\"Num classes:   {cfg.num_classes}\")\n",
    "print(f\"Class names:   {cfg.class_names}\")\n",
    "print(f\"Train split:   {cfg.train_split}\")\n",
    "print(f\"Val split:     {cfg.val_split}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Explore Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Load raw dataset (no transforms) for exploration\n",
    "train_raw = YOLODetectionDataset(root=cfg.root, split=cfg.train_split)\n",
    "val_raw = YOLODetectionDataset(root=cfg.root, split=cfg.val_split)\n",
    "\n",
    "print(f\"Train images: {len(train_raw)}\")\n",
    "print(f\"Val images:   {len(val_raw)}\")\n",
    "\n",
    "# Count class distribution across training set\n",
    "class_counts = Counter()\n",
    "for i in range(len(train_raw)):\n",
    "    _, targets = train_raw.load_raw(i)\n",
    "    for t in targets:\n",
    "        class_counts[int(t[0])] += 1\n",
    "\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "for cls_id, count in sorted(class_counts.items()):\n",
    "    name = cfg.class_names[cls_id] if cls_id < len(cfg.class_names) else f\"class_{cls_id}\"\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "# Visualize a few samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "for ax, idx in zip(axes, range(4)):\n",
    "    img, targets = train_raw.load_raw(idx)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for t in targets:\n",
    "        cls_id, cx, cy, bw, bh = t\n",
    "        x1 = int((cx - bw / 2) * w)\n",
    "        y1 = int((cy - bh / 2) * h)\n",
    "        x2 = int((cx + bw / 2) * w)\n",
    "        y2 = int((cy + bh / 2) * h)\n",
    "        color = [(0, 255, 0), (255, 0, 0), (0, 0, 255)][int(cls_id) % 3]\n",
    "        cv2.rectangle(img_rgb, (x1, y1), (x2, y2), color, 2)\n",
    "        label = cfg.class_names[int(cls_id)] if int(cls_id) < len(cfg.class_names) else str(int(cls_id))\n",
    "        cv2.putText(img_rgb, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "    ax.imshow(img_rgb)\n",
    "    ax.set_title(f\"Image {idx} ({len(targets)} objects)\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Fine-Tune YOLO-NAS S\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from modern_yolonas import yolo_nas_s\n",
    "from modern_yolonas.data import YOLODetectionDataset\n",
    "from modern_yolonas.data.transforms import (\n",
    "    Compose,\n",
    "    HSVAugment,\n",
    "    HorizontalFlip,\n",
    "    LetterboxResize,\n",
    "    Normalize,\n",
    ")\n",
    "from modern_yolonas.data.collate import detection_collate_fn\n",
    "from modern_yolonas.training.trainer import Trainer\n",
    "\n",
    "# Build model with pretrained COCO backbone, random head for our classes\n",
    "model = yolo_nas_s(pretrained=True, num_classes=cfg.num_classes)\n",
    "print(f\"Model created: YOLO-NAS S with {cfg.num_classes} classes\")\n",
    "print(\"Backbone+neck weights: loaded from COCO pretrained\")\n",
    "print(\"Head cls_pred layers: randomly initialized\")\n",
    "\n",
    "# Transforms\n",
    "train_transforms = Compose([\n",
    "    HSVAugment(),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    LetterboxResize(target_size=640),\n",
    "    Normalize(),\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    LetterboxResize(target_size=640),\n",
    "    Normalize(),\n",
    "])\n",
    "\n",
    "# Datasets & loaders\n",
    "train_ds = YOLODetectionDataset(\n",
    "    root=cfg.root, split=cfg.train_split, transforms=train_transforms\n",
    ")\n",
    "val_ds = YOLODetectionDataset(\n",
    "    root=cfg.root, split=cfg.val_split, transforms=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=8, shuffle=True, num_workers=2,\n",
    "    collate_fn=detection_collate_fn, pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=8, shuffle=False, num_workers=2,\n",
    "    collate_fn=detection_collate_fn, pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_classes=cfg.num_classes,\n",
    "    epochs=15,\n",
    "    lr=2e-4,\n",
    "    warmup_steps=100,\n",
    "    output_dir=\"runs/hardhat\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Evaluate & Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "from modern_yolonas import Detector\n",
    "\n",
    "# Load best checkpoint\n",
    "checkpoint = torch.load(\"runs/hardhat/last.pt\", map_location=\"cpu\", weights_only=True)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detector = Detector(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    conf_threshold=0.25,\n",
    "    iou_threshold=0.45,\n",
    "    retain_image=True,\n",
    ")\n",
    "\n",
    "# Run inference on validation images\n",
    "val_images = sorted((cfg.root / \"images\" / cfg.val_split).glob(\"*.*\"))[:8]\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "class_detection_counts = Counter()\n",
    "\n",
    "for ax, img_path in zip(axes.flat, val_images):\n",
    "    detection = detector.detect_image(str(img_path))\n",
    "    vis = detection.visualize(class_names=cfg.class_names)\n",
    "    ax.imshow(cv2.cvtColor(vis, cv2.COLOR_BGR2RGB))\n",
    "    ax.set_title(f\"{len(detection.boxes)} detections\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    for cls_id in detection.class_ids:\n",
    "        name = cfg.class_names[int(cls_id)] if int(cls_id) < len(cfg.class_names) else f\"class_{int(cls_id)}\"\n",
    "        class_detection_counts[name] += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetection counts per class:\")\n",
    "for name, count in class_detection_counts.most_common():\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Export to ONNX\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "export_model = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\n",
    "export_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "export_model.eval()\n",
    "\n",
    "# Fuse RepVGG blocks for deployment\n",
    "for module in export_model.modules():\n",
    "    if hasattr(module, \"fuse_block_residual_branches\"):\n",
    "        module.fuse_block_residual_branches()\n",
    "\n",
    "dummy = torch.randn(1, 3, 640, 640)\n",
    "onnx_path = \"runs/hardhat/model_float32.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    export_model,\n",
    "    dummy,\n",
    "    onnx_path,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"pred_bboxes\", \"pred_scores\"],\n",
    "    dynamic_axes={\n",
    "        \"images\": {0: \"batch\"},\n",
    "        \"pred_bboxes\": {0: \"batch\"},\n",
    "        \"pred_scores\": {0: \"batch\"},\n",
    "    },\n",
    "    opset_version=17,\n",
    ")\n",
    "\n",
    "onnx_size = Path(onnx_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"ONNX exported to: {onnx_path}\")\n",
    "print(f\"Float32 model size: {onnx_size:.1f} MB\")\n",
    "\n",
    "# Verify output shapes\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "outputs = session.run(None, {\"images\": dummy.numpy()})\n",
    "print(f\"Output shapes: bboxes={outputs[0].shape}, scores={outputs[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Post-Training Quantization (PTQ)\n",
    "from modern_yolonas.quantization import (\n",
    "    prepare_model_ptq,\n",
    "    run_calibration,\n",
    "    convert_quantized,\n",
    "    export_quantized_onnx,\n",
    ")\n",
    "\n",
    "# Prepare model for PTQ\n",
    "ptq_model_fresh = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\n",
    "ptq_model_fresh.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "ptq_model_fresh.eval()\n",
    "\n",
    "ptq_model = prepare_model_ptq(ptq_model_fresh)\n",
    "print(\"PTQ model prepared with observers\")\n",
    "\n",
    "# Calibrate on validation set (small set is fine for PTQ)\n",
    "run_calibration(ptq_model, val_loader, num_batches=20, device=\"cpu\")\n",
    "print(\"Calibration complete\")\n",
    "\n",
    "# Convert to quantized model\n",
    "quantized_model = convert_quantized(ptq_model)\n",
    "print(\"Model converted to int8\")\n",
    "\n",
    "# Export quantized ONNX\n",
    "ptq_onnx_path = \"runs/hardhat/model_ptq_int8.onnx\"\n",
    "export_quantized_onnx(quantized_model, ptq_onnx_path, input_size=640)\n",
    "\n",
    "ptq_size = Path(ptq_onnx_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nPTQ ONNX exported to: {ptq_onnx_path}\")\n",
    "print(f\"PTQ model size: {ptq_size:.1f} MB\")\n",
    "print(f\"Size reduction: {(1 - ptq_size / onnx_size) * 100:.0f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Quantization-Aware Training (QAT)\n",
    "from modern_yolonas.quantization import prepare_model_qat\n",
    "from modern_yolonas.quantization.qat_trainer import QATTrainer\n",
    "\n",
    "# Prepare model for QAT from the fine-tuned checkpoint\n",
    "qat_model_fresh = yolo_nas_s(pretrained=False, num_classes=cfg.num_classes)\n",
    "qat_model_fresh.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "qat_model = prepare_model_qat(qat_model_fresh)\n",
    "print(\"QAT model prepared with fake-quantization nodes\")\n",
    "\n",
    "# QAT fine-tuning (short, low LR)\n",
    "qat_trainer = QATTrainer(\n",
    "    model=qat_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_classes=cfg.num_classes,\n",
    "    epochs=5,\n",
    "    lr=2e-5,\n",
    "    warmup_steps=50,\n",
    "    output_dir=\"runs/hardhat/qat\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "qat_trainer.train()\n",
    "print(\"QAT training complete!\")\n",
    "\n",
    "# Convert QAT model and export\n",
    "qat_quantized = convert_quantized(qat_model)\n",
    "qat_onnx_path = \"runs/hardhat/model_qat_int8.onnx\"\n",
    "export_quantized_onnx(qat_quantized, qat_onnx_path, input_size=640)\n",
    "\n",
    "qat_size = Path(qat_onnx_path).stat().st_size / (1024 * 1024)\n",
    "print(f\"\\nQAT ONNX exported to: {qat_onnx_path}\")\n",
    "print(f\"\\nModel size comparison:\")\n",
    "print(f\"  Float32: {onnx_size:.1f} MB\")\n",
    "print(f\"  PTQ:     {ptq_size:.1f} MB ({(1 - ptq_size / onnx_size) * 100:.0f}% smaller)\")\n",
    "print(f\"  QAT:     {qat_size:.1f} MB ({(1 - qat_size / onnx_size) * 100:.0f}% smaller)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: ONNX Runtime Inference\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modern_yolonas.inference.preprocess import preprocess\n",
    "from modern_yolonas.inference.postprocess import postprocess, rescale_boxes\n",
    "from modern_yolonas.inference.visualize import draw_detections\n",
    "\n",
    "# Load ONNX model\n",
    "session = ort.InferenceSession(onnx_path)\n",
    "\n",
    "# Load a test image\n",
    "test_image_path = str(val_images[0])\n",
    "original = cv2.imread(test_image_path)\n",
    "orig_h, orig_w = original.shape[:2]\n",
    "\n",
    "# Preprocess\n",
    "input_tensor, pad_info = preprocess(original, input_size=640)\n",
    "\n",
    "# Run inference\n",
    "pred_bboxes, pred_scores = session.run(\n",
    "    None, {\"images\": input_tensor.numpy()}\n",
    ")\n",
    "\n",
    "# Postprocess\n",
    "import torch\n",
    "boxes, scores, class_ids = postprocess(\n",
    "    torch.from_numpy(pred_bboxes),\n",
    "    torch.from_numpy(pred_scores),\n",
    "    conf_threshold=0.25,\n",
    "    iou_threshold=0.45,\n",
    ")\n",
    "\n",
    "# Rescale boxes to original image size\n",
    "if len(boxes) > 0:\n",
    "    boxes = rescale_boxes(boxes, pad_info, (orig_h, orig_w))\n",
    "\n",
    "# Visualize\n",
    "annotated = draw_detections(\n",
    "    original, boxes.numpy(), scores.numpy(), class_ids.numpy(),\n",
    "    class_names=cfg.class_names,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f\"ONNX Runtime Inference ({len(boxes)} detections)\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDetections:\")\n",
    "for i in range(len(boxes)):\n",
    "    cls_name = cfg.class_names[int(class_ids[i])] if int(class_ids[i]) < len(cfg.class_names) else f\"class_{int(class_ids[i])}\"\n",
    "    print(f\"  {cls_name}: {scores[i]:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
